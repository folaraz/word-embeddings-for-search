{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import operator\n",
    "from nltk import ngrams\n",
    "import gc\n",
    "\n",
    "\n",
    "def load_doc(file):\n",
    "    text = open(file, 'r').read()\n",
    "    return text\n",
    "\n",
    "def get_sentences(file):\n",
    "    doc = load_doc(file)\n",
    "    sentences = sent_tokenize(doc)\n",
    "    return sentences\n",
    "\n",
    "def clean_doc(file):\n",
    "    docs = get_sentences(file)\n",
    "    result = []\n",
    "    for doc in docs:\n",
    "        doc = doc.replace('--', ' ')\n",
    "        tokens = doc.split()\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        result.append(tokens)\n",
    "    return result\n",
    "\n",
    "def get_phrase_vec(model,tok_sent):\n",
    "    numerator = np.zeros(100)\n",
    "    for word in tok_sent:\n",
    "        try:\n",
    "            vec = model[word]\n",
    "        except Exception as e:\n",
    "            vec = np.zeros(100)\n",
    "        numerator += vec\n",
    "    return numerator/len(tok_sent)\n",
    "        \n",
    "def avg_sentence( model,file):\n",
    "    c_docs = clean_doc(file)\n",
    "    sent_vec = []\n",
    "    for doc in c_docs:\n",
    "        numerator = np.zeros(100)\n",
    "        denominator = 0\n",
    "        for word in doc:\n",
    "            try:\n",
    "                vec = model[word]\n",
    "            except KeyError as e:\n",
    "                vec = np.zeros(100)\n",
    "            numerator += vec\n",
    "        sent_vec.append((doc,numerator/len(doc)))\n",
    "    return sent_vec\n",
    "\n",
    "def search_word(model, input_word,textFile,Phrase=True):\n",
    "    vector_sentences = avg_sentence(model,textFile)\n",
    "    if Phrase:\n",
    "        word_vec = get_phrase_vec(model, input_word.split(' '))\n",
    "    else:\n",
    "        try:\n",
    "            word_vec = model[input_word]\n",
    "        except KeyError as e:\n",
    "            raise ValueError(\"Word not present in the vocabulary\")\n",
    "    n = 0\n",
    "    ans = []\n",
    "    for vec in vector_sentences:\n",
    "        calc_vec = list(cosine_similarity(word_vec.reshape(1, -1),vec[1].reshape(1, -1)))[0][0]\n",
    "        ans.append((' '.join(vec[0]),calc_vec))\n",
    "    sent_with_highest_signal = sorted(ans, key = lambda x: x[1])[-1][0]\n",
    "    uni_to_trigrams = [(' '.join(c),cosine_similarity(get_phrase_vec(model,c).reshape(1,-1),word_vec.reshape(1,-1))[0][0])\n",
    "                                         for i in range(1,4) for c in ngrams(sent_with_highest_signal.split(),i)]\n",
    "    print('-----------------------------THE MAIN SENTENCE------------------------------------------------------')\n",
    "    print(sent_with_highest_signal)\n",
    "    print('-----------------------------THE NGRAMS ALONGSIDE THEIR RESPECTIVE SCORES---------------------------')\n",
    "    return sorted(uni_to_trigrams, key = lambda x: x[1])[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "google_negative_news =  '/Users/abdulrazzaq/gensim-data/GoogleNews-vectors-negative300.bin'\n",
    "model=KeyedVectors.load_word2vec_format('model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------THE MAIN SENTENCE------------------------------------------------------\n",
      "expect some combination of domestically developed payments schemes andor countries allowing global payments operators ie\n",
      "-----------------------------THE NGRAMS ALONGSIDE THEIR RESPECTIVE SCORES---------------------------\n",
      "[('developed payments schemes', 0.2819372226792563), ('developed payments', 0.282412453539377), ('allowing global payments', 0.2996403382999625), ('domestically developed payments', 0.306298603285112), ('global payments', 0.3299203234273732)]\n"
     ]
    }
   ],
   "source": [
    "ans = search_word(model,'soaring revenue','article.txt',Phrase=True)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINACIAL NEW CUSTOM WORD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('finance_articles.csv')\n",
    "df2 = pd.read_csv('financial_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_sent(df1,df2,filename):  \n",
    "    total = df1.Text.tolist() + df2.Text.tolist()\n",
    "    data = '\\n'.join(total)\n",
    "    file = open(filename,'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_doc_sent(df1,df2,'financial_news.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_sentences('financial_news.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_financial_news(sentences):\n",
    "    result = []\n",
    "    for doc in sentences:\n",
    "        doc = doc.replace('\\n',' ')\n",
    "        doc = doc.replace('--', ' ')\n",
    "        tokens = doc.split()\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        result.append(tokens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_news = clean_financial_news(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['zacks', 'analysts', 'anticipate', 'ultragenyx', 'pharmaceutical', 'inc', 'rare', 'will', 'announce', 'quarterly', 'sales', 'of', 'million', 'posted', 'by', 'maurice', 'goldstein', 'on', 'may', 'equities', 'research', 'analysts', 'forecast', 'that', 'ultragenyx', 'pharmaceutical', 'inc', 'nasdaqrare', 'will', 'post', 'sales', 'of', 'million', 'for', 'the', 'current', 'fiscal', 'quarter', 'according', 'to', 'zacks'], ['seven', 'analysts', 'have', 'issued', 'estimates', 'for', 'ultragenyx', 'earnings', 'with', 'the', 'lowest', 'sales', 'estimate', 'coming', 'in', 'at', 'million', 'and', 'the', 'highest', 'estimate', 'coming', 'in', 'at', 'million'], ['ultragenyx', 'pharmaceutical', 'reported', 'sales', 'of', 'million', 'in', 'the', 'same', 'quarter', 'last', 'year', 'which', 'indicates', 'a', 'positive', 'yearoveryear', 'growth', 'rate', 'of'], ['the', 'business', 'is', 'scheduled', 'to', 'issue', 'its', 'next', 'earnings', 'report', 'on', 'thursday', 'august']]\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_news[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "EMB_DIM = 100\n",
    "\n",
    "w2v = Word2Vec(cleaned_news, size=EMB_DIM, window=5,min_count=5,negative=15,iter=10,workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('billion', 0.8768537640571594),\n",
       " ('percent', 0.5705723166465759),\n",
       " ('total', 0.5587888956069946),\n",
       " ('trillion', 0.51272052526474),\n",
       " ('crores', 0.49851346015930176),\n",
       " ('millions', 0.4977300465106964),\n",
       " ('net', 0.488506019115448),\n",
       " ('eur', 0.4871695637702942),\n",
       " ('sek', 0.48228564858436584),\n",
       " ('year', 0.47676876187324524)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = w2v.wv\n",
    "word_vectors.similar_by_word('million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('debts', 0.6732707023620605),\n",
       " ('borrowings', 0.598007321357727),\n",
       " ('loan', 0.5769705772399902),\n",
       " ('loans', 0.5721713304519653),\n",
       " ('liquidity', 0.5491695404052734),\n",
       " ('interest', 0.5271838307380676),\n",
       " ('borrowing', 0.5213406682014465),\n",
       " ('bonds', 0.5212751626968384),\n",
       " ('cash', 0.5018055438995361),\n",
       " ('bond', 0.499487966299057)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word('debt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('revenues', 0.6219348907470703),\n",
       " ('revenue', 0.6071063280105591),\n",
       " ('arpu', 0.5076402425765991),\n",
       " ('yearonyear', 0.4826663136482239),\n",
       " ('inventories', 0.47095710039138794),\n",
       " ('consumption', 0.46385905146598816),\n",
       " ('liquor', 0.4626953899860382),\n",
       " ('profit', 0.45940881967544556),\n",
       " ('exports', 0.4573214054107666),\n",
       " ('apac', 0.4533824920654297)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word('sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1723237]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "cosine_similarity(w2v['bank'].reshape(1, -1), w2v['revenue'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5769705]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(w2v['debt'].reshape(1, -1), w2v['loan'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.save_word2vec_format('model.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
