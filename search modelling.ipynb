{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import operator\n",
    "from nltk import ngrams\n",
    "import gc\n",
    "\n",
    "\n",
    "def load_doc(file):\n",
    "    text = open(file, 'r').read()\n",
    "    return text\n",
    "\n",
    "def get_sentences(file):\n",
    "    doc = load_doc(file)\n",
    "    sentences = sent_tokenize(doc)\n",
    "    return sentences\n",
    "\n",
    "def clean_doc(file):\n",
    "    docs = get_sentences(file)\n",
    "    result = []\n",
    "    for doc in docs:\n",
    "        doc = doc.replace('--', ' ')\n",
    "        tokens = doc.split()\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        result.append(tokens)\n",
    "    return result\n",
    "\n",
    "def get_phrase_vec(model,tok_sent):\n",
    "    numerator = np.zeros(300)\n",
    "    for word in tok_sent:\n",
    "        try:\n",
    "            vec = model[word]\n",
    "        except Exception as e:\n",
    "            vec = np.zeros(300)\n",
    "        numerator += vec\n",
    "    return numerator/len(tok_sent)\n",
    "        \n",
    "def avg_sentence( model,file):\n",
    "    c_docs = clean_doc(file)\n",
    "    sent_vec = []\n",
    "    for doc in c_docs:\n",
    "        numerator = np.zeros(300)\n",
    "        denominator = 0\n",
    "        for word in doc:\n",
    "            try:\n",
    "                vec = model[word]\n",
    "            except KeyError as e:\n",
    "                vec = np.zeros(300)\n",
    "            numerator += vec\n",
    "        sent_vec.append((doc,numerator/len(doc)))\n",
    "    return sent_vec\n",
    "\n",
    "def search_word(model, input_word,textFile,Phrase=True):\n",
    "    vector_sentences = avg_sentence(model,textFile)\n",
    "    if Phrase:\n",
    "        word_vec = get_phrase_vec(model, input_word.split(' '))\n",
    "    else:\n",
    "        try:\n",
    "            word_vec = model[input_word]\n",
    "        except KeyError as e:\n",
    "            raise ValueError(\"Word not present in the vocabulary\")\n",
    "    n = 0\n",
    "    ans = []\n",
    "    for vec in vector_sentences:\n",
    "        calc_vec = list(cosine_similarity(word_vec.reshape(1, -1),vec[1].reshape(1, -1)))[0][0]\n",
    "        ans.append((' '.join(vec[0]),calc_vec))\n",
    "    sent_with_highest_signal = sorted(ans, key = lambda x: x[1])[-1][0]\n",
    "    uni_to_trigrams = [(' '.join(c),cosine_similarity(get_phrase_vec(model,c).reshape(1,-1),word_vec.reshape(1,-1))[0][0])\n",
    "                                         for i in range(1,4) for c in ngrams(sent_with_highest_signal.split(),i)]\n",
    "    print('-----------------------------THE MAIN SENTENCE------------------------------------------------------')\n",
    "    print(sent_with_highest_signal)\n",
    "    print('-----------------------------THE NGRAMS ALONGSIDE THEIR RESPECTIVE SCORES---------------------------')\n",
    "    return sorted(uni_to_trigrams, key = lambda x: x[1])[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model=KeyedVectors.load_word2vec_format(\"/Users/abdulrazzaq/gensim-data/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------THE MAIN SENTENCE------------------------------------------------------\n",
      "the firm has a market cap of billion a pe ratio of and a beta of\n",
      "-----------------------------THE NGRAMS ALONGSIDE THEIR RESPECTIVE SCORES---------------------------\n",
      "[('and a beta', 0.31662712170703106), ('a beta of', 0.31662712170703106), ('beta', 0.31662712170703117), ('a beta', 0.31662712170703117), ('beta of', 0.31662712170703117)]\n"
     ]
    }
   ],
   "source": [
    "ans = search_word(model,'uber about to ipo','article.txt',Phrase=True)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINACIAL NEW CUSTOM WORD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('finance_articles.csv')\n",
    "df2 = pd.read_csv('financial_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_sent(df1,df2,filename):  \n",
    "    total = df1.Text.tolist() + df2.Text.tolist()\n",
    "    data = '\\n'.join(total)\n",
    "    file = open(filename,'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_doc_sent(df1,df2,'financial_news.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_sentences('financial_news.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_financial_news(sentences):\n",
    "    result = []\n",
    "    for doc in sentences:\n",
    "        doc = doc.replace('\\n',' ')\n",
    "        doc = doc.replace('--', ' ')\n",
    "        tokens = doc.split()\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        result.append(tokens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_news = clean_financial_news(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['zacks', 'analysts', 'anticipate', 'ultragenyx', 'pharmaceutical', 'inc', 'rare', 'will', 'announce', 'quarterly', 'sales', 'of', 'million', 'posted', 'by', 'maurice', 'goldstein', 'on', 'may', 'equities', 'research', 'analysts', 'forecast', 'that', 'ultragenyx', 'pharmaceutical', 'inc', 'nasdaqrare', 'will', 'post', 'sales', 'of', 'million', 'for', 'the', 'current', 'fiscal', 'quarter', 'according', 'to', 'zacks'], ['seven', 'analysts', 'have', 'issued', 'estimates', 'for', 'ultragenyx', 'earnings', 'with', 'the', 'lowest', 'sales', 'estimate', 'coming', 'in', 'at', 'million', 'and', 'the', 'highest', 'estimate', 'coming', 'in', 'at', 'million'], ['ultragenyx', 'pharmaceutical', 'reported', 'sales', 'of', 'million', 'in', 'the', 'same', 'quarter', 'last', 'year', 'which', 'indicates', 'a', 'positive', 'yearoveryear', 'growth', 'rate', 'of'], ['the', 'business', 'is', 'scheduled', 'to', 'issue', 'its', 'next', 'earnings', 'report', 'on', 'thursday', 'august']]\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_news[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "EMB_DIM = 300\n",
    "\n",
    "w2v = Word2Vec(cleaned_news, size=EMB_DIM, window=5,min_count=5,negative=15,iter=10,workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('billion', 0.8043133616447449),\n",
       " ('percent', 0.481628954410553),\n",
       " ('total', 0.45313483476638794),\n",
       " ('net', 0.42137467861175537),\n",
       " ('trillion', 0.3999459743499756),\n",
       " ('per', 0.39225542545318604),\n",
       " ('millions', 0.38231557607650757),\n",
       " ('year', 0.36807942390441895),\n",
       " ('approximately', 0.36258527636528015),\n",
       " ('tons', 0.35911741852760315)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = w2v.wv\n",
    "word_vectors.similar_by_word('million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
